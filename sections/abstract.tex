\abstract{
Social media has become a central arena for public discourse, prompting platforms to implement content moderation policies to reduce harmful or hateful speech. Yet as these systems have grown in complexity, user concerns about transparency, fairness, and censorship have intensified. This paper investigates how users’ understanding (or lack thereof) of moderation methods affects the perceived legitimacy of these systems and, consequently, user behavior on social platforms. Through an extensive review of existing academic literature, we synthesize findings across case studies and empirical research to identify three overarching sources of user frustration: the obscurity of moderation policies, the misalignment between platform decisions and user expectations, and the inconsistent application of rules. Our analysis demonstrates that these gaps in comprehension and trust undermine platforms’ stated goals for equitable and effective moderation. Although this paper does not present new empirical research, it offers a comprehensive overview of current scholarship on user perceptions of social media moderation and recommendations to enhance the clarity, fairness, and transparency of moderation practices. We argue that content moderation is only legitimate when users understand, trust, and accept the policies that govern their online expression.
}
